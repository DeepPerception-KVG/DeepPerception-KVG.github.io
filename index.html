<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Advancing Cognitive Visual Perception in MLLMs for Knowledge-Intensive Visual Grounding">
  <meta name="keywords" content="Knowledge-Intensive Visual Grounding">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title> DeepPerception</title>



  <!-- <link rel="icon" href="./static/images/logo.png"> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/leaderboard.css">

  <!-- <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bulma.min.css" rel="stylesheet">
  <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script> -->
  <script type="text/javascript" src="static/js/sort-table.js" defer></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/explorer-index.js"></script>
  <script src="./static/js/question_card.js"></script>

  <script src="./static/js/leaderboard_testmini.js"></script>  
  <script src="./data/results/output_folders.js" defer></script>
  <script src="./data/results/model_scores.js" defer></script>

  <script src="./visualizer/data/data_public.js" defer></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
  <script src="https://kit.fontawesome.com/65fb0bea81.js" crossorigin="anonymous"></script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true
      }
  });
  </script>
  <!-- <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script> -->
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title is-bold">
            <span class="DeepPerception" style="font-family: 'CustomFont', sans-serif;">DeepPerception</span>:
            </h1>
          <h2 class="subtitle is-3 publication-subtitle">
            Advancing Cognitive Visual Perception in MLLMs for Knowledge-Intensive Visual Grounding
          </h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://openreview.net/profile?id=~Xinyu_Ma2">Xinyu Ma</a><sup style="color:#d62828;">1</sup>,
            </span>
            <span class="author-block">
              <a href="https://openreview.net/profile?id=~Ziyang_Ding1">Ziyang Ding</a><sup style="color:#f77f00;">2</sup>,
            </span>
            <span class="author-block">
              <a href="https://openreview.net/profile?id=~Zhicong_Luo2">Zhicong Luo</a><sup style="color:#fcbf49">3</sup>,
            </span>
            <span class="author-block">
              <a href="https://openreview.net/profile?id=~Chi_Chen1">Chi Chen</a><sup style="color:#2a9d8f;">4</sup>,
            </span>
            <span class="author-block">
              <a href="https://openreview.net/profile?id=~Zonghao_Guo1">Zonghao Guo</a><sup style="color:#2a9d8f">4</sup>,
            </span><br>
            <span class="author-block">
              <a href="https://openreview.net/profile?id=~Derek_F._Wong1">Derek F. Wong</a><sup style="color:#d62828;">1</sup>,
            </span>
            <span class="author-block">
              <a href="https://openreview.net/profile?id=~Xiaoyi_Feng4">Xiaoyi Feng</a><sup style="color:#f77f00">3</sup>,
            </span>
            <span class="author-block">
              <a href="https://openreview.net/profile?id=~Maosong_Sun1">Maosong Sun</a><sup style="color:#2a9d8f">4</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup style="color:#d62828">1</sup>University of Macau,</span>
            <span class="author-block"><sup style="color:#f77f00;">2</sup>Shandong University,</span><br>
            <span class="author-block"><sup style="color:#fcbf49">3</sup>Northwestern Polytechnical University,</span>
            <span class="author-block"><sup style="color:#2a9d8f">4</sup>Tsinghua University</span><br>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/MaxyLee/DeepPerception"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Demo Link. -->
              <!-- <span class="link-block">
                <a href="https://huggingface.co/Michael4933/Migician"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <p style="font-size:18px">üñºÔ∏è</p>
                  </span>
                  <span>Demo</span>
                </a>
              </span> -->
              <!-- Model Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <p style="font-size:18px">ü§ó</p>
                  </span>
                  <span>Model</span>
                </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/MaxyLee/KVG"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <i class="fa-solid fa-database"></i>
                  </span>
                  <span>Data</span>
                </a>
              </span> 
              <span class="link-block">
                <a href="https://huggingface.co/datasets/MaxyLee/KVG-Bench"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>KVG-Bench</span>
                </a>
              </span> 
              <!-- Benchmark Link. -->
              <!-- <span class="link-block">
                <a href="https://huggingface.co/datasets/Michael4933/MIG-Bench"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <p style="font-size:18px">ü§ó</p>
                  </span>
                  <span>Benchmark</span>
                </a>
              </span> -->


            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
    <div class="container" style="margin-top: -150px; margin-bottom: -170px;">
      <div class="columns is-centered m-6">
        <div class="column is-full has-text-centered content_index">
        <img src="static/images/header.png " type="image/png">
        <div style="height: 15px;"></div> <!-- Âõ∫ÂÆöÈ´òÂ∫¶Á©∫Ë°å -->
        <p class="subtitle has-text-centered caption">
            Figure 1: (a) DeepPerception employs knowledge-driven reasoning to derive answers, while the baseline model directly outputs predictions without cognitive processing. 
            (b) DeepPerception demonstrates superior cognitive visual perception capabilities that cannot be elicited in the foundation model through simplistic zero-shot CoT prompting.
        </p>
        </div>
    </div>
  </section>
  
<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <div class="columns is-centered m-6">
        <div class="column is-full has-text-centered content_index">
        <h2 class="title is-3">Abstract</h2>
        <div class="content_index has-text-justified">
          <p style="width: 1000px; margin: 0 auto;">
            Human experts excel at fine-grained visual discrimination by leveraging domain knowledge to refine perceptual features, a capability that remains underdeveloped in current Multimodal Large Language Models (MLLMs). Despite possessing vast expert-level knowledge, MLLMs struggle to integrate reasoning into visual perception, often generating direct responses without deeper analysis. <br>
            To bridge this gap, we introduce knowledge-intensive visual grounding (KVG), a novel visual grounding task that requires both finegrained perception and domain-specific knowledge integration. To address the challenges of KVG, we propose <strong>DeepPerception, an MLLM enhanced with cognitive visual perception capabilities</strong>. Our approach consists of (1) an automated data synthesis pipeline that generates high-quality, knowledge-aligned training samples, and (2) a two-stage training framework combining supervised fine-tuning for cognitive reasoning scaffolding and reinforcement learning to optimize perceptioncognition synergy. To benchmark performance, we introduce KVG-Bench, a comprehensive dataset spanning <strong>10</strong> domains with <strong>1.3K</strong> manually curated test cases. <br>
            Experimental results demonstrate that DeepPerception significantly outperforms direct fine-tuning, achieving <strong>+8.08%</strong> accuracy improvements on KVG-Bench and exhibiting <strong>+4.60%</strong> superior cross-domain generalization over baseline approaches. Our findings highlight the importance of integrating cognitive processes into MLLMs for human-like visual perception and open new directions for multimodal reasoning research.

          </p>
        </div>
      </div>
    </div>
</div>
</section>


<!-- --------------------------------------Benchmark------------------------------------------------- -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class="title is-1 deepperception2">KVG-Bench</h1>
  </div>
</section>

<section class="section">
    <div class="container" style="margin-top: -60px; margin-bottom: -100px;">
      <div class="columns is-centered m-6">
        <div class="column is-full has-text-centered content">
        <img src="static/images/benchmark.png " type="image/png">
        <div style="height: 15px;"></div> <!-- Âõ∫ÂÆöÈ´òÂ∫¶Á©∫Ë°å -->
        <p class="subtitle has-text-centered caption">
          Figure 2: (a) KVG-Bench images contain multiple subordinate-category entities (e.g., Boeing 777, 767, 757, 747, 737, 727, 717, 707 from top to bottom in the left image); (b) KVG-Bench exhibits high diversity across categories and entities.
        </p>
        <div style="height: 30px;"></div> <!-- Âõ∫ÂÆöÈ´òÂ∫¶Á©∫Ë°å -->
        <h2 class="title is-3">Task Definition</h2>
        <div class="content_index has-text-justified">
            <p>
                The task of knowledge-intensive visual grounding (KVG) is to predict a bounding box $B = f_\theta (X_I , X_T )$ through joint understanding of visual input $X_I$ and textual query $X_T$ , requiring fine-grained alignment between multimodal representations. While sharing structural similarities with referring expression comprehension (REC), this task significantly elevates the challenge beyond standard REC tasks. As exemplified in Figure above, the queries of KVG involve fine-grained entity specifications (e.g., ‚ÄúBoeing 747‚Äù) rather than generic categories such as ‚Äúaircraft‚Äù. Additionally, each image contains multiple objects from the same category of the target object (e.g., multiple aircraft in a single image). This setup requires both <strong>expert-level knowledge and advanced perceptual and reasoning abilities</strong> to pinpoint the precise features that distinguish the target from similar objects.
            </p>
        </div>
        <h2 class="title is-3">Human Evaluation</h2>
        <div class="content_index has-text-justified">
            <p>
                To assess the diÔ¨Äiculty of KVG-Bench, we conducted human evaluations with 11 non-expert volunteers under two experimental settings: Closed-Book (no external resources) and Open-Book (single Wikipedia query/case to simulate expert-level knowledge integration). Participants were randomly assigned several categories, with each category at least five evaluators to mitigate knowledge bias. The evaluation results, as shown in Tab. 1, reveal significant performance differences between settings. The Open-Book Setting demonstrated significant performance elevation (78.83% accuracy) compared to Closed-Book results (56.41%). This validates that KVG-Bench requires synergistic integration of expert-level knowledge and fine-grained visual comparison, positioning it as a meaningful testbed for advancing cognitive visual perception in MLLMs.
            </p>
        </div>
      </div>
    </div>
  </section>


<!-- --------------------------------------Methods------------------------------------------------- -->

<p style="margin-bottom: 100px;"></p>

<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class="title is-1 deepperception2">Method</h1>
  </div>
</section>

<section class="section">
  <div class="container" style="margin-top: -45px; margin-bottom: -100px;">
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content_index">
        <!-- <h2 class="title is-3">Overview</h2> -->
        <img src="static/images/method.png " type="image/png">
        <div style="height: 15px;"></div>
        <p class="subtitle has-text-centered caption">
          Figure 3: Overview of the proposed data engine and two-stage training framework.
        </p>
      </div>
    </div>
</section>

<!-- <section class="section">
  <div class="container" style="margin-top: -45px; margin-bottom: -100px;">
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content_index">
        <h2 class="title is-3">Data Engine</h2>
        <div class="content_index has-text-justified">
          <p>
            The training data images for KVG must meet the following criteria: (1) they should include expert-level entity and bounding box annotations, and (2) the images should contain multiple similar objects to ensure the difficulty of the task.
            To overcome the scarcity of such data, we developed an automated pipeline leveraging FGVR datasets as the data source.
            First, data from multiple FGVR datasets were categorized into ten categories. 
            The first five categories were used directly for training, while the last five were used solely as unseen categories, with their data not being used for training. 
            Since there are no bounding box annotations in the original FGVR datasets, we employed Qwen2VL-7B to generate bounding boxes through entity-specific prompts (e.g., "Find and give the bounding box of Airbus A330").
            With entity labels pre-verified for presence, this automated method achieved over 95% accuracy, validated through manual checks.
            To overcome the limitation of single-object dominance in existing datasets, as illustrated in the above figure, we synthesized composite images containing at least 2 entities from the same category (e.g., multiple dog species in one image) using horizontal, vertical, grid, or random layouts, adjusting bounding box coordinates to preserve annotation consistency while enforcing entity uniqueness. 
            This approach requires models to perform fine-grained visual comparisons through structured cognitive processes rather than relying on memorization.
            The final dataset, partitioned for two-stage training, balances real-world complexity with quality control, simulating scenarios demanding cognitive visual perception.
          </p>
      </div>
      </div>
    </div>
</section> -->

<section class="section">
  <div class="container" style="margin-top: -45px; margin-bottom: -100px;">
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content_index">
        <h2 class="title is-3">Chain-of-Thought Supervised Fine-Tuning</h2>
        <img src="static/images/figure-CoT.png " type="image/png">
        <p class="subtitle has-text-centered caption">
          Figure 4: Example of Chain-of-Thought data generated by Qwen2-VL-72B.
        </p>
        <div style="height: 30px;"></div> <!-- Âõ∫ÂÆöÈ´òÂ∫¶Á©∫Ë°å -->
        <div class="content_index has-text-justified">
          <p>
            The primary objective of the first stage training is to endow models with visual perception-oriented cognitive capabilities through synthesized CoT training data. 
            This process fosters an initial cognitive-perceptual synergy by explicitly modeling reasoning trajectories in the KVG task while establishing fundamental knowledge association patterns for subsequent optimization.
            Specifically, we leveraged the powerful Qwen2-VL-72B model to generate CoT reasoning data.
            As illustrated in the figures, we generated CoT rationales by inputting images, ground-truth annotations (labels and bounding boxes), and CoT reasoning prompts into the model.
            This process synthesizes reasoning chains that combine the model's domain knowledge with human-like cognitive processes (e.g., iterative hypothesis verification through joint domain knowledge and visual feature analysis).
            We then performed SFT on the Qwen2-VL-7B model using these data, yielding the stage-1 model with foundational cognitive visual perception capabilities. 
            This model demonstrates preliminary abilities to integrate domain knowledge and generate stepwise reasoning chains for visual grounding, which serves as a stable foundation for the second-stage reinforcement learning training.
            This approach bridges the gap between raw perception and expert-level cognition, enabling models to emulate human-like visual perception through knowledge-visual co-verification.
          </p>
      </div>
      </div>
    </div>
</section>


<section class="section">
  <div class="container" style="margin-top: -45px; margin-bottom: -100px;">
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content_index">
        <h2 class="title is-3">Perception-oriented Reinforcement Learning</h2>
        <!-- <img src="static/images/figure-CoT.png " type="image/png"> -->
        <!-- <p class="subtitle has-text-centered caption">
          Case study using Chain-of-Thought data generated by Qwen2-VL-72B.
        </p> -->
        <!-- <div style="height: 30px;"></div> Âõ∫ÂÆöÈ´òÂ∫¶Á©∫Ë°å -->
        <div class="content_index has-text-justified">
          <p>
            Following the initial CoT-SFT training, we conducted
            reinforcement learning using a separate subset of training data to further enhance the model's perception
            capabilities, building upon its acquired cognitive visual perception foundation. As illustrated in the overview of the framework, we
            adopted GRPO following Guo et al, which optimizes the policy model by sampling a group of outputs
            to a question and calculating a group-relative advantage instead of a critic model. To adapt GRPO for visual grounding tasks, we designed a rule-based reward
            system and performed data filtering on the training data using the stage-1 model.
          </p>
      </div>
      
      <div style="height: 15px;"></div>
      <div class="content_index has-text-justified">
        <p>
          <strong>Training Objective.</strong> Let $q$ denote the question and ${o_1, . . . , o_G}$ denote the sampled outputs from the old policy model œÄŒ∏old , then the optimization objective of
          policy model $\pi_{\theta}$ can be formally defined as: 
        </p>
        <p align="center">$\mathcal{J}_{G R P O}(\theta)=\mathbb{E}\left[q \sim P(Q),\left\{o_i\right\}_{i=1}^G \sim \pi_{\theta_{o l d}}(O \mid q)\right]\frac{1}{G} \sum_{i=1}^G \left(\frac{\pi_\theta\left(o_i \mid q\right)}{\pi_{\theta_{o l d}}\left(o_i \mid q\right)} A_i - \beta \mathbb{D}_{K L}\left(\pi_\theta| | \pi_{r e f}\right)\right),$</p>
        <p align="center">$\mathbb{D}_{K L}\left(\pi_\theta| | \pi_{r e f}\right)=\frac{\pi_{r e f}\left(o_i \mid q\right)}{\pi_\theta\left(o_i \mid q\right)}-\log \frac{\pi_{r e f}\left(o_i \mid q\right)}{\pi_\theta\left(o_i \mid q\right)}-1$</p>
        <p>where $\varepsilon$ and $\beta$ are hyper-parameters and $A_i$ is the advantage computed based on a group of rewards $\{r_1, \ldots, r_G\}$ corresponding to the outputs of each group:</p>
        <p align="center">$A_i=\frac{r_i-mean(\{r_1, \ldots, r_G\})}{std(\{r_1, \ldots, r_G\})}$</p>
      </div>

      <div style="height: 25px;"></div>
      <div class="content_index has-text-justified">
        <p>
          <strong>Reward Modeling.</strong> We adopted a rule-based reward system specifically tailored for visual grounding tasks, comprising two types of rewards: Intersection over Union (IoU) reward and Format reward.
          The IoU reward evaluates the spatial alignment between predicted bounding boxes and ground-truth annotations.
          Given the ground-truth bounding box $B$ and the predicted bounding box $\tilde{B}$, the IoU reward is formally defined as:
        </p>
        <p align="center">$R_{\mathrm{IoU}}= 
          \begin{cases}
          \operatorname{IoU}(B, \tilde{B}), & \text { if } \operatorname{IoU}(B, \tilde{B}) \geq \tau \\ 
          0, & \text { otherwise }
          \end{cases}$</p>
        
      </div>
    </div>
  </div>
</section>



<p style="margin-bottom: 100px;"></p>


<!-- --------------------------------------Experiments Results------------------------------------------------- -->


<p style="margin-bottom: 100px;"></p>

<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class="title is-1 deepperception2">Experiment & Analysis</h1>
  </div>
</section>

<section class="section">
    <div class="container" style="margin-top: -45px; margin-bottom: -100px;">
      <div class="columns is-centered m-6">
        <div class="column is-full has-text-centered content_index">
          <h2 class="title is-3">Main Results</h2>
          <img src="static/images/main_result.png " type="image/png">
          <div style="height: 15px;"></div>
          <p class="subtitle has-text-centered caption">
            Table 1: KVG-Bench reults of DeepPerception and baseline models.
          </p>
          <div style="height: 30px;"></div> <!-- Âõ∫ÂÆöÈ´òÂ∫¶Á©∫Ë°å -->
          <div class="content_index has-text-justified">
            <p>
                As demonstrated in Tab. 1, our DeepPerception model achieves significant performance improvements by integrating cognitive visual perception capabilities, validating the hypothesis that human-inspired cognition-perception synergy enhances visual perception. On in-domain categories, the model attains 63.13% average accuracy, surpassing all 7B-scale baselines (e.g., +10.5% over SFT) and outperforming 70B-scale models such as InternVL2-Llama3-76B (59.22%). For out-of-domain generalization, it achieves 60.85% accuracy ‚Äîcomparable to 70B-scale models‚Äîwhile demonstrating near-human performance in categories where the model possesses richer knowledge, such as Dog (85.60% vs. human evaluators‚Äô78.89%).<br>
                These results confirm that DeepPerception‚Äôs success stems from its ability to emulate human-like cognitive processes: iterative knowledge-guided reasoning refines perceptual hypotheses (e.g., verifying anatomical features stepwise), while reinforcement learning aligns these hypotheses with precise visual outputs. The consistent superiority over non-cognitive baselines, particularly in fine-grained categories like Car (86.54% vs. human 85.56%), proves that structured knowledge integration‚Äînot mere visual memorization ‚Äîdrives performance gains. By bridging knowledge reasoning with perception, DeepPerception establishes new state-of-the-art results on KVG-Bench, demonstrating that cognitive mechanisms central to biological vision can be effectively operationalized in multimodal AI systems.
            </p>
            </div>

          <div style="height: 30px;"></div> <!-- Âõ∫ÂÆöÈ´òÂ∫¶Á©∫Ë°å -->

          <img src="static/images/fgvr.png " type="image/png">
          <div style="height: 15px;"></div>
          <p class="subtitle has-text-centered caption">
            Table 2: FGVR reults of DeepPerception and baseline models.
          </p>

          <div class="content_index has-text-justified">
            <p>
                To validate the generalizability of cognitive visual perception, we conducted experiments on FGVR datasets. Unlike the two-stage training paradigm used for KVG, only the reinforcement learning phase was applied here, as base models already exhibit strong baseline performance on FGVR. As shown in Tab. 2, our DeepPerception model achieves state-of-the-art results across all categories, demonstrating universal performance gains: DeepPerception achieves state-of-the-art results across all FGVR categories (83.21% average accuracy), outperforming Qwen2-VL-7B by +3.64%. This evidence strongly supports that cognitive visual perception, characterized by iterative knowledge-visual alignment, provides universal benefits for fine-grained perception, mirroring human experts‚Äô ability to combine sensory input with conceptual understanding.
            </p>
            </div>

          <div style="height: 30px;"></div> <!-- Âõ∫ÂÆöÈ´òÂ∫¶Á©∫Ë°å -->

          <img src="static/images/general.png " type="image/png">
          <div style="height: 15px;"></div>
          <p class="subtitle has-text-centered caption">
            Table 3: Performance Comparison of DeepPerception and Qwen2VL-7B on general multimodal benchmarks.
          </p>
          <div class="content_index has-text-justified">
            <p>
                To comprehensively assess the model‚Äôs general capabilities across diverse multimodal scenarios, we conducted evaluations on established multimodal benchmarks. As shown in Tab. 3, DeepPerception maintains performance comparable to the base model, demonstrating preserved general capabilities without degradation.
            </p>
            </div>
        </div>
      </div>
  </section>


<!-- <section class="section">
    <div class="container" style="margin-top: -60px; margin-bottom: -100px;">
      <div class="columns is-centered m-6">
        <div class="column is-full has-text-centered content_index">

          <div id="results-carousel" class="carousel results-carousel">

            <div class="box">
              <div class="content_index has-text-centered">
                <img src="static/images/main_result.png" type="image/png">
              </div>
            </div>

            <div class="box">
              <div class="content_index has-text-centered">
                <img src="static/images/fgvr.png" type="image/png">
              </div>
            </div>

            <div class="box">
                <div class="content_index has-text-centered">
                  <img src="static/images/general.png" type="image/png">
                </div>
            </div>
            
          </div>
        </div>
      </div>
    </div>
</section> -->

  <p style="margin-bottom: 100px;"></p>


<!-- --------------------------------------Case Study------------------------------------------------- -->


<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class="title is-1 deepperception2">Cases Study</h1>
  </div>
</section>

<section class="section">
  <div class="container" style="margin-top: -45px; margin-bottom: -100px;">
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content_index">
        <img src="static/images/KVG-case.png " type="image/png">
        <div style="height: 15px;"></div>
        <p class="subtitle has-text-centered caption">
          Case study comparing DeepPerception and Qwen2-VL-7B on KVG-Bench.
        </p>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container" style="margin-top: -45px; margin-bottom: -100px;">
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content_index">
        <img src="static/images/FOCI-case.png " type="image/png">
        <div style="height: 15px;"></div>
        <p class="subtitle has-text-centered caption">
          Case study comparing DeepPerception and Qwen2-VL-7B on FGVR.
        </p>
      </div>
    </div>
  </div>
</section>


<!-- --------------------------------------Citation------------------------------------------------- -->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content_index">
    <h2 class="title is-3 has-text-centered">Citation</h2>
    <pre><code>@misc{,
    title={}, 
    author={},
    year={2025},
    url={}, 
}</code></pre>
  </div>
</section>


<footer class="footer">
    <div class="content_index has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a href="https://nerfies.github.io/">Nerfies</a>, licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
</footer>

</body>
</html>